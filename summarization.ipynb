{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import json\n",
    "import librosa\n",
    "import pytesseract\n",
    "import soundfile\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import notebook_login\n",
    "from pytesseract import Output\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import create_optimizer, AdamWeightDecay, TFAutoModelForSeq2SeqLM\n",
    "from transformers.keras_callbacks import KerasMetricCallback, PushToHubCallback\n",
    "from transformers.pipelines.pt_utils import KeyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('config.json')\n",
    "data = json.load(f)\n",
    "f.close()\n",
    "tokenizer = data['write_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "430c587cbfeb4f73939335f539a4ad91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Load Dataset](https://huggingface.co/docs/transformers/tasks/summarization#load-billsum-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "billsum = load_dataset(\"billsum\", split=\"ca_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "billsum = billsum.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Preprocess](https://huggingface.co/docs/transformers/tasks/summarization#preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"google-t5/t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"summarize: \"\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"text\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
    "\n",
    "    labels = tokenizer(text_target=examples[\"summary\"], max_length=128, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4cc7284adae469aa5bbb112614d4e88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/989 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc0733ba12794469805198aa2ac29c97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/248 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_billsum = billsum.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### pytorch\n",
    "# data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)\n",
    "\n",
    "### tensorflow\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Evaluate](https://huggingface.co/docs/transformers/tasks/summarization#load-billsum-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Train](https://huggingface.co/docs/transformers/tasks/summarization#train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sindhumadhavan/GitHub/transformers/.venv/lib/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TFT5ForConditionalGeneration' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 15\u001b[0m\n\u001b[1;32m      1\u001b[0m training_args \u001b[38;5;241m=\u001b[39m Seq2SeqTrainingArguments(\n\u001b[1;32m      2\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_awesome_billsum_model\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     push_to_hub\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 15\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mSeq2SeqTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_billsum\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_billsum\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/GitHub/transformers/.venv/lib/python3.9/site-packages/transformers/trainer_seq2seq.py:57\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m     44\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     45\u001b[0m     model: Union[\u001b[39m\"\u001b[39m\u001b[39mPreTrainedModel\u001b[39m\u001b[39m\"\u001b[39m, nn\u001b[39m.\u001b[39mModule] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m     preprocess_logits_for_metrics: Optional[Callable[[torch\u001b[39m.\u001b[39mTensor, torch\u001b[39m.\u001b[39mTensor], torch\u001b[39m.\u001b[39mTensor]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     56\u001b[0m ):\n\u001b[0;32m---> 57\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m     58\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m     59\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m     60\u001b[0m         data_collator\u001b[39m=\u001b[39;49mdata_collator,\n\u001b[1;32m     61\u001b[0m         train_dataset\u001b[39m=\u001b[39;49mtrain_dataset,\n\u001b[1;32m     62\u001b[0m         eval_dataset\u001b[39m=\u001b[39;49meval_dataset,\n\u001b[1;32m     63\u001b[0m         tokenizer\u001b[39m=\u001b[39;49mtokenizer,\n\u001b[1;32m     64\u001b[0m         model_init\u001b[39m=\u001b[39;49mmodel_init,\n\u001b[1;32m     65\u001b[0m         compute_metrics\u001b[39m=\u001b[39;49mcompute_metrics,\n\u001b[1;32m     66\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m     67\u001b[0m         optimizers\u001b[39m=\u001b[39;49moptimizers,\n\u001b[1;32m     68\u001b[0m         preprocess_logits_for_metrics\u001b[39m=\u001b[39;49mpreprocess_logits_for_metrics,\n\u001b[1;32m     69\u001b[0m     )\n\u001b[1;32m     71\u001b[0m     \u001b[39m# Override self.model.generation_config if a GenerationConfig is specified in args.\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     \u001b[39m# Priority: args.generation_config > model.generation_config > default GenerationConfig.\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgeneration_config \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/GitHub/transformers/.venv/lib/python3.9/site-packages/transformers/trainer.py:495\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[39m# Bnb Quantized models doesn't support `.to` operation.\u001b[39;00m\n\u001b[1;32m    491\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    492\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplace_model_on_device\n\u001b[1;32m    493\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(model, \u001b[39m\"\u001b[39m\u001b[39mquantization_method\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m QuantizationMethod\u001b[39m.\u001b[39mBITS_AND_BYTES\n\u001b[1;32m    494\u001b[0m ):\n\u001b[0;32m--> 495\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_move_model_to_device(model, args\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m    497\u001b[0m \u001b[39m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[39;00m\n\u001b[1;32m    498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_model_parallel:\n",
      "File \u001b[0;32m~/GitHub/transformers/.venv/lib/python3.9/site-packages/transformers/trainer.py:736\u001b[0m, in \u001b[0;36mTrainer._move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_move_model_to_device\u001b[39m(\u001b[39mself\u001b[39m, model, device):\n\u001b[0;32m--> 736\u001b[0m     model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m    737\u001b[0m     \u001b[39m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[39;00m\n\u001b[1;32m    738\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mparallel_mode \u001b[39m==\u001b[39m ParallelMode\u001b[39m.\u001b[39mTPU \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(model, \u001b[39m\"\u001b[39m\u001b[39mtie_weights\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TFT5ForConditionalGeneration' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"my_awesome_billsum_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=4,\n",
    "    predict_with_generate=True,\n",
    "    # fp16=True,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "# trainer = Seq2SeqTrainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_billsum[\"train\"],\n",
    "#     eval_dataset=tokenized_billsum[\"test\"],\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=data_collator,\n",
    "#     compute_metrics=compute_metrics,\n",
    "# )\n",
    "\n",
    "# trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    tokenized_billsum[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_test_set = model.prepare_tf_dataset(\n",
    "    tokenized_billsum[\"test\"],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer)  # No loss argument!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sindhumadhavan/GitHub/transformers/.venv/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\n",
      "For more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Cloning https://huggingface.co/sanoosha94/my_awesome_billsum_model into local empty directory.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "push_to_hub_callback = PushToHubCallback(\n",
    "    output_dir=\"my_awesome_billsum_model\",\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [metric_callback, push_to_hub_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:AutoGraph could not transform <function infer_framework at 0x159aa4430> and will run it as-is.\n",
      "Cause: for/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function infer_framework at 0x159aa4430> and will run it as-is.\n",
      "Cause: for/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "61/61 [==============================] - ETA: 0s - loss: 3.6362  "
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/sindhumadhavan/GitHub/transformers/.venv/lib/python3.9/site-packages/tf_keras/src/engine/training.py\", line 2436, in predict_function  *\n        return step_function(self, iterator)\n    File \"/Users/sindhumadhavan/GitHub/transformers/.venv/lib/python3.9/site-packages/tf_keras/src/engine/training.py\", line 2409, in run_step  *\n        outputs = model.predict_step(data)\n    File \"/Users/sindhumadhavan/GitHub/transformers/.venv/lib/python3.9/site-packages/tf_keras/src/engine/training.py\", line 2377, in predict_step  *\n        return self(x, training=False)\n    File \"/Users/sindhumadhavan/GitHub/transformers/.venv/lib/python3.9/site-packages/tf_keras/src/engine/training.py\", line 558, in error_handler  *\n        return fn(*args, **kwargs)\n    File \"/Users/sindhumadhavan/GitHub/transformers/.venv/lib/python3.9/site-packages/tf_keras/src/engine/training.py\", line 588, in __call__  *\n        return super().__call__(*args, **kwargs)\n    File \"/Users/sindhumadhavan/GitHub/transformers/.venv/lib/python3.9/site-packages/tf_keras/src/engine/training.py\", line 558, in error_handler  *\n        return fn(*args, **kwargs)\n    File \"/Users/sindhumadhavan/GitHub/transformers/.venv/lib/python3.9/site-packages/tf_keras/src/engine/base_layer.py\", line 1136, in __call__  *\n        outputs = call_fn(inputs, *args, **kwargs)\n    File \"/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_file98v_l434.py\", line 162, in error_handler\n        raise ag__.converted_call(ag__.ld(new_e).with_traceback, (ag__.ld(e).__traceback__,), None, fscope_1) from None\n    File \"/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_file98v_l434.py\", line 34, in error_handler\n        retval__1 = ag__.converted_call(ag__.ld(fn), tuple(ag__.ld(args)), dict(**ag__.ld(kwargs)), fscope_1)\n    File \"/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_filettvklrq4.py\", line 37, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_fileoa_8znbw.py\", line 65, in tf__call\n        ag__.if_stmt(ag__.and_(lambda : ag__.ld(input_ids) is not None, lambda : ag__.ld(inputs_embeds) is not None), if_body_2, else_body_2, get_state_2, set_state_2, ('input_ids', 'input_shape'), 2)\n    File \"/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_fileoa_8znbw.py\", line 62, in else_body_2\n        ag__.if_stmt(ag__.ld(input_ids) is not None, if_body_1, else_body_1, get_state_1, set_state_1, ('input_ids', 'input_shape'), 2)\n    File \"/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_fileoa_8znbw.py\", line 59, in else_body_1\n        ag__.if_stmt(ag__.ld(inputs_embeds) is not None, if_body, else_body, get_state, set_state, ('input_shape',), 1)\n    File \"/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_fileoa_8znbw.py\", line 56, in else_body\n        raise ag__.converted_call(ag__.ld(ValueError), (f'You have to specify either {ag__.ld(err_msg_prefix)}input_ids or {ag__.ld(err_msg_prefix)}inputs_embeds',), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'tft5_for_conditional_generation' (type TFT5ForConditionalGeneration).\n    \n    in user code:\n    \n        File \"/Users/sindhumadhavan/GitHub/transformers/.venv/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\", line 1404, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/Users/sindhumadhavan/GitHub/transformers/.venv/lib/python3.9/site-packages/transformers/models/t5/modeling_tf_t5.py\", line 1464, in call  *\n            decoder_outputs = self.decoder(\n        File \"/Users/sindhumadhavan/GitHub/transformers/.venv/lib/python3.9/site-packages/tf_keras/src/engine/training.py\", line 558, in error_handler  *\n            return fn(*args, **kwargs)\n        File \"/Users/sindhumadhavan/GitHub/transformers/.venv/lib/python3.9/site-packages/tf_keras/src/engine/base_layer.py\", line 1136, in __call__  *\n            outputs = call_fn(inputs, *args, **kwargs)\n        File \"/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_file98v_l434.py\", line 162, in error_handler  **\n            raise ag__.converted_call(ag__.ld(new_e).with_traceback, (ag__.ld(e).__traceback__,), None, fscope_1) from None\n        File \"/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_file98v_l434.py\", line 34, in error_handler\n            retval__1 = ag__.converted_call(ag__.ld(fn), tuple(ag__.ld(args)), dict(**ag__.ld(kwargs)), fscope_1)\n        File \"/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_filettvklrq4.py\", line 37, in tf__run_call_with_unpacked_inputs  **\n            retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n        File \"/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_fileoa_8znbw.py\", line 65, in tf__call  **\n            ag__.if_stmt(ag__.and_(lambda : ag__.ld(input_ids) is not None, lambda : ag__.ld(inputs_embeds) is not None), if_body_2, else_body_2, get_state_2, set_state_2, ('input_ids', 'input_shape'), 2)\n        File \"/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_fileoa_8znbw.py\", line 62, in else_body_2\n            ag__.if_stmt(ag__.ld(input_ids) is not None, if_body_1, else_body_1, get_state_1, set_state_1, ('input_ids', 'input_shape'), 2)\n        File \"/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_fileoa_8znbw.py\", line 59, in else_body_1\n            ag__.if_stmt(ag__.ld(inputs_embeds) is not None, if_body, else_body, get_state, set_state, ('input_shape',), 1)\n        File \"/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_fileoa_8znbw.py\", line 56, in else_body\n            raise ag__.converted_call(ag__.ld(ValueError), (f'You have to specify either {ag__.ld(err_msg_prefix)}input_ids or {ag__.ld(err_msg_prefix)}inputs_embeds',), None, fscope)\n    \n        ValueError: Exception encountered when calling layer 'decoder' (type TFT5MainLayer).\n        \n        in user code:\n        \n            File \"/Users/sindhumadhavan/GitHub/transformers/.venv/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\", line 1404, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"/Users/sindhumadhavan/GitHub/transformers/.venv/lib/python3.9/site-packages/transformers/models/t5/modeling_tf_t5.py\", line 763, in call  *\n                raise ValueError(f\"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds\")\n        \n            ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds\n        \n        \n        Call arguments received by layer 'decoder' (type TFT5MainLayer):\n          â€¢ input_ids=None\n          â€¢ attention_mask=None\n          â€¢ encoder_hidden_states=tf.Tensor(shape=(16, 1024, 512), dtype=float32)\n          â€¢ encoder_attention_mask=tf.Tensor(shape=(16, 1024), dtype=int32)\n          â€¢ inputs_embeds=None\n          â€¢ head_mask=None\n          â€¢ encoder_head_mask=None\n          â€¢ past_key_values=None\n          â€¢ use_cache=True\n          â€¢ output_attentions=False\n          â€¢ output_hidden_states=False\n          â€¢ return_dict=True\n          â€¢ training=False\n    \n    \n    Call arguments received by layer 'tft5_for_conditional_generation' (type TFT5ForConditionalGeneration):\n      â€¢ input_ids={'input_ids': 'tf.Tensor(shape=(16, 1024), dtype=int64)', 'attention_mask': 'tf.Tensor(shape=(16, 1024), dtype=int64)'}\n      â€¢ attention_mask=None\n      â€¢ decoder_input_ids=None\n      â€¢ decoder_attention_mask=None\n      â€¢ head_mask=None\n      â€¢ decoder_head_mask=None\n      â€¢ encoder_outputs=None\n      â€¢ past_key_values=None\n      â€¢ inputs_embeds=None\n      â€¢ decoder_inputs_embeds=None\n      â€¢ labels=None\n      â€¢ use_cache=None\n      â€¢ output_attentions=None\n      â€¢ output_hidden_states=None\n      â€¢ return_dict=None\n      â€¢ training=False\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf_train_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf_test_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHub/transformers/.venv/lib/python3.9/site-packages/transformers/modeling_tf_utils.py:1170\u001b[0m, in \u001b[0;36mTFPreTrainedModel.fit\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(keras\u001b[39m.\u001b[39mModel\u001b[39m.\u001b[39mfit)\n\u001b[1;32m   1168\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1169\u001b[0m     args, kwargs \u001b[39m=\u001b[39m convert_batch_encoding(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 1170\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/GitHub/transformers/.venv/lib/python3.9/site-packages/tf_keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/GitHub/transformers/.venv/lib/python3.9/site-packages/transformers/keras_callbacks.py:229\u001b[0m, in \u001b[0;36mKerasMetricCallback.on_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    225\u001b[0m         predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mgenerate(\n\u001b[1;32m    226\u001b[0m             generation_inputs, attention_mask\u001b[39m=\u001b[39mattention_mask, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate_kwargs\n\u001b[1;32m    227\u001b[0m         )\n\u001b[1;32m    228\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 229\u001b[0m     predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict_on_batch(batch)\n\u001b[1;32m    230\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(predictions, \u001b[39mdict\u001b[39m):\n\u001b[1;32m    231\u001b[0m         \u001b[39m# This converts any dict-subclass to a regular dict\u001b[39;00m\n\u001b[1;32m    232\u001b[0m         \u001b[39m# Keras REALLY doesn't like it when we pass around a BatchEncoding or other derived class\u001b[39;00m\n\u001b[1;32m    233\u001b[0m         predictions \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(predictions)\n",
      "File \u001b[0;32m~/GitHub/transformers/.venv/lib/python3.9/site-packages/transformers/modeling_tf_utils.py:1185\u001b[0m, in \u001b[0;36mTFPreTrainedModel.predict_on_batch\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1182\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(keras\u001b[39m.\u001b[39mModel\u001b[39m.\u001b[39mpredict_on_batch)\n\u001b[1;32m   1183\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict_on_batch\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1184\u001b[0m     args, kwargs \u001b[39m=\u001b[39m convert_batch_encoding(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 1185\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mpredict_on_batch(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_file3ws70lv2.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_filedy2virtt.py:45\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__step_function\u001b[0;34m(model, iterator)\u001b[0m\n\u001b[1;32m     43\u001b[0m ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mjit_compile, if_body, else_body, get_state, set_state, (\u001b[39m'\u001b[39m\u001b[39mrun_step\u001b[39m\u001b[39m'\u001b[39m,), \u001b[39m1\u001b[39m)\n\u001b[1;32m     44\u001b[0m data \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mnext\u001b[39m), (ag__\u001b[39m.\u001b[39mld(iterator),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 45\u001b[0m outputs \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(model)\u001b[39m.\u001b[39mdistribute_strategy\u001b[39m.\u001b[39mrun, (ag__\u001b[39m.\u001b[39mld(run_step),), \u001b[39mdict\u001b[39m(args\u001b[39m=\u001b[39m(ag__\u001b[39m.\u001b[39mld(data),)), fscope)\n\u001b[1;32m     46\u001b[0m outputs \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(reduce_per_replica), (ag__\u001b[39m.\u001b[39mld(outputs), ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mdistribute_strategy), \u001b[39mdict\u001b[39m(reduction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mconcat\u001b[39m\u001b[39m'\u001b[39m), fscope)\n\u001b[1;32m     47\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_filedy2virtt.py:18\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__step_function.<locals>.run_step\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     16\u001b[0m do_return_1 \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     17\u001b[0m retval__1 \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m---> 18\u001b[0m outputs \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(model)\u001b[39m.\u001b[39mpredict_step, (ag__\u001b[39m.\u001b[39mld(data),), \u001b[39mNone\u001b[39;00m, fscope_1)\n\u001b[1;32m     19\u001b[0m \u001b[39mwith\u001b[39;00m ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mcontrol_dependencies(ag__\u001b[39m.\u001b[39mld(_minimum_control_deps)(ag__\u001b[39m.\u001b[39mld(outputs))):\n\u001b[1;32m     20\u001b[0m     ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(model)\u001b[39m.\u001b[39m_predict_counter\u001b[39m.\u001b[39massign_add, (\u001b[39m1\u001b[39m,), \u001b[39mNone\u001b[39;00m, fscope_1)\n",
      "File \u001b[0;32m/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_file576it4df.py:32\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     31\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), (ag__\u001b[39m.\u001b[39mld(x),), \u001b[39mdict\u001b[39m(training\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m), fscope)\n\u001b[1;32m     33\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_fileom70pxmd.py:44\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m         filtered_tb \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mfiltered_tb\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     43\u001b[0m filtered_tb \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mfiltered_tb\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 44\u001b[0m ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mnot_(ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mdebugging\u001b[39m.\u001b[39mis_traceback_filtering_enabled, (), \u001b[39mNone\u001b[39;00m, fscope)), if_body, else_body, get_state, set_state, (\u001b[39m'\u001b[39m\u001b[39mdo_return\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mretval_\u001b[39m\u001b[39m'\u001b[39m), \u001b[39m2\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[39mreturn\u001b[39;00m fscope\u001b[39m.\u001b[39mret(retval_, do_return)\n",
      "File \u001b[0;32m/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_fileom70pxmd.py:40\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__error_handler.<locals>.else_body\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     39\u001b[0m     filtered_tb \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(_process_traceback_frames), (ag__\u001b[39m.\u001b[39mld(e)\u001b[39m.\u001b[39m__traceback__,), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 40\u001b[0m     \u001b[39mraise\u001b[39;00m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(e)\u001b[39m.\u001b[39mwith_traceback, (ag__\u001b[39m.\u001b[39mld(filtered_tb),), \u001b[39mNone\u001b[39;00m, fscope) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     filtered_tb \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mfiltered_tb\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_fileom70pxmd.py:34\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__error_handler.<locals>.else_body\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(fn), \u001b[39mtuple\u001b[39m(ag__\u001b[39m.\u001b[39mld(args)), \u001b[39mdict\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mag__\u001b[39m.\u001b[39mld(kwargs)), fscope)\n\u001b[1;32m     35\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_filekh4hhqg9.py:67\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf____call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     66\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39msuper\u001b[39m), (), \u001b[39mNone\u001b[39;00m, fscope)\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m, \u001b[39mtuple\u001b[39m(ag__\u001b[39m.\u001b[39mld(args)), \u001b[39mdict\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mag__\u001b[39m.\u001b[39mld(kwargs)), fscope)\n\u001b[1;32m     68\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_fileom70pxmd.py:44\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m         filtered_tb \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mfiltered_tb\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     43\u001b[0m filtered_tb \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mfiltered_tb\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 44\u001b[0m ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mnot_(ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mdebugging\u001b[39m.\u001b[39mis_traceback_filtering_enabled, (), \u001b[39mNone\u001b[39;00m, fscope)), if_body, else_body, get_state, set_state, (\u001b[39m'\u001b[39m\u001b[39mdo_return\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mretval_\u001b[39m\u001b[39m'\u001b[39m), \u001b[39m2\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[39mreturn\u001b[39;00m fscope\u001b[39m.\u001b[39mret(retval_, do_return)\n",
      "File \u001b[0;32m/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_fileom70pxmd.py:40\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__error_handler.<locals>.else_body\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     39\u001b[0m     filtered_tb \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(_process_traceback_frames), (ag__\u001b[39m.\u001b[39mld(e)\u001b[39m.\u001b[39m__traceback__,), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 40\u001b[0m     \u001b[39mraise\u001b[39;00m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(e)\u001b[39m.\u001b[39mwith_traceback, (ag__\u001b[39m.\u001b[39mld(filtered_tb),), \u001b[39mNone\u001b[39;00m, fscope) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     filtered_tb \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mfiltered_tb\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_fileom70pxmd.py:34\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__error_handler.<locals>.else_body\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(fn), \u001b[39mtuple\u001b[39m(ag__\u001b[39m.\u001b[39mld(args)), \u001b[39mdict\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mag__\u001b[39m.\u001b[39mld(kwargs)), fscope)\n\u001b[1;32m     35\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_filezlsyltsb.py:242\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf____call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    240\u001b[0m mask_is_implicit \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mmask_is_implicit\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    241\u001b[0m training_mode \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mtraining_mode\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 242\u001b[0m ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(_in_functional_construction_mode), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(inputs), ag__\u001b[39m.\u001b[39mld(args), ag__\u001b[39m.\u001b[39mld(kwargs), ag__\u001b[39m.\u001b[39mld(input_list)), \u001b[39mNone\u001b[39;00m, fscope), if_body_11, else_body_11, get_state_11, set_state_11, (\u001b[39m'\u001b[39m\u001b[39mdo_return\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mkwargs[\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmask\u001b[39m\u001b[39m'\u001b[39m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mretval_\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39margs\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39minput_list\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39minputs\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mkwargs\u001b[39m\u001b[39m'\u001b[39m), \u001b[39m3\u001b[39m)\n\u001b[1;32m    243\u001b[0m \u001b[39mreturn\u001b[39;00m fscope\u001b[39m.\u001b[39mret(retval_, do_return)\n",
      "File \u001b[0;32m/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_filezlsyltsb.py:187\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf____call__.<locals>.else_body_11\u001b[0;34m()\u001b[0m\n\u001b[1;32m    185\u001b[0m ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m_autocast, if_body_7, else_body_7, get_state_7, set_state_7, (\u001b[39m'\u001b[39m\u001b[39minputs\u001b[39m\u001b[39m'\u001b[39m,), \u001b[39m1\u001b[39m)\n\u001b[1;32m    186\u001b[0m \u001b[39mwith\u001b[39;00m ag__\u001b[39m.\u001b[39mld(autocast_variable)\u001b[39m.\u001b[39menable_auto_cast_variables(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m_compute_dtype_object):\n\u001b[0;32m--> 187\u001b[0m     outputs \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(call_fn), (ag__\u001b[39m.\u001b[39mld(inputs),) \u001b[39m+\u001b[39m \u001b[39mtuple\u001b[39m(ag__\u001b[39m.\u001b[39mld(args)), \u001b[39mdict\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mag__\u001b[39m.\u001b[39mld(kwargs)), fscope)\n\u001b[1;32m    189\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_state_8\u001b[39m():\n\u001b[1;32m    190\u001b[0m     \u001b[39mreturn\u001b[39;00m ()\n",
      "File \u001b[0;32m/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_file98v_l434.py:162\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__inject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m     display_name \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mdisplay_name\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    161\u001b[0m     ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mld(arguments_context), if_body_5, else_body_5, get_state_6, set_state_6, (\u001b[39m'\u001b[39m\u001b[39mnew_e\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39marguments_context\u001b[39m\u001b[39m'\u001b[39m), \u001b[39m1\u001b[39m)\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mraise\u001b[39;00m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(new_e)\u001b[39m.\u001b[39mwith_traceback, (ag__\u001b[39m.\u001b[39mld(e)\u001b[39m.\u001b[39m__traceback__,), \u001b[39mNone\u001b[39;00m, fscope_1) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    164\u001b[0m     signature \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39msignature\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_file98v_l434.py:34\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__inject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     do_return_1 \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m     retval__1 \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(fn), \u001b[39mtuple\u001b[39m(ag__\u001b[39m.\u001b[39mld(args)), \u001b[39mdict\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mag__\u001b[39m.\u001b[39mld(kwargs)), fscope_1)\n\u001b[1;32m     35\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     do_return_1 \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_filettvklrq4.py:37\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(func), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m),), \u001b[39mdict\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mag__\u001b[39m.\u001b[39mld(unpacked_inputs)), fscope)\n\u001b[1;32m     38\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_filebxifig_j.py:91\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m     90\u001b[0m ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mand_(\u001b[39mlambda\u001b[39;00m : ag__\u001b[39m.\u001b[39mld(labels) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39mlambda\u001b[39;00m : ag__\u001b[39m.\u001b[39mand_(\u001b[39mlambda\u001b[39;00m : ag__\u001b[39m.\u001b[39mld(decoder_input_ids) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39mlambda\u001b[39;00m : ag__\u001b[39m.\u001b[39mld(decoder_inputs_embeds) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)), if_body_2, else_body_2, get_state_2, set_state_2, (\u001b[39m'\u001b[39m\u001b[39mdecoder_input_ids\u001b[39m\u001b[39m'\u001b[39m,), \u001b[39m1\u001b[39m)\n\u001b[0;32m---> 91\u001b[0m decoder_outputs \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mdecoder, (ag__\u001b[39m.\u001b[39mld(decoder_input_ids),), \u001b[39mdict\u001b[39m(attention_mask\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(decoder_attention_mask), encoder_hidden_states\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(hidden_states), encoder_attention_mask\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(attention_mask), inputs_embeds\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(decoder_inputs_embeds), head_mask\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(decoder_head_mask), past_key_values\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(past_key_values), use_cache\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(use_cache), output_attentions\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(output_attentions), output_hidden_states\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(output_hidden_states), return_dict\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(return_dict), training\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(training)), fscope)\n\u001b[1;32m     92\u001b[0m sequence_output \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(decoder_outputs)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     94\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_state_3\u001b[39m():\n",
      "File \u001b[0;32m/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_fileom70pxmd.py:44\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m         filtered_tb \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mfiltered_tb\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     43\u001b[0m filtered_tb \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mfiltered_tb\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 44\u001b[0m ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mnot_(ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mdebugging\u001b[39m.\u001b[39mis_traceback_filtering_enabled, (), \u001b[39mNone\u001b[39;00m, fscope)), if_body, else_body, get_state, set_state, (\u001b[39m'\u001b[39m\u001b[39mdo_return\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mretval_\u001b[39m\u001b[39m'\u001b[39m), \u001b[39m2\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[39mreturn\u001b[39;00m fscope\u001b[39m.\u001b[39mret(retval_, do_return)\n",
      "File \u001b[0;32m/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_fileom70pxmd.py:40\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__error_handler.<locals>.else_body\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     39\u001b[0m     filtered_tb \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(_process_traceback_frames), (ag__\u001b[39m.\u001b[39mld(e)\u001b[39m.\u001b[39m__traceback__,), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 40\u001b[0m     \u001b[39mraise\u001b[39;00m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(e)\u001b[39m.\u001b[39mwith_traceback, (ag__\u001b[39m.\u001b[39mld(filtered_tb),), \u001b[39mNone\u001b[39;00m, fscope) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     filtered_tb \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mfiltered_tb\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_fileom70pxmd.py:34\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__error_handler.<locals>.else_body\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(fn), \u001b[39mtuple\u001b[39m(ag__\u001b[39m.\u001b[39mld(args)), \u001b[39mdict\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mag__\u001b[39m.\u001b[39mld(kwargs)), fscope)\n\u001b[1;32m     35\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_filezlsyltsb.py:242\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf____call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    240\u001b[0m mask_is_implicit \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mmask_is_implicit\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    241\u001b[0m training_mode \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mtraining_mode\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 242\u001b[0m ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(_in_functional_construction_mode), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(inputs), ag__\u001b[39m.\u001b[39mld(args), ag__\u001b[39m.\u001b[39mld(kwargs), ag__\u001b[39m.\u001b[39mld(input_list)), \u001b[39mNone\u001b[39;00m, fscope), if_body_11, else_body_11, get_state_11, set_state_11, (\u001b[39m'\u001b[39m\u001b[39mdo_return\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mkwargs[\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmask\u001b[39m\u001b[39m'\u001b[39m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mretval_\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39margs\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39minput_list\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39minputs\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mkwargs\u001b[39m\u001b[39m'\u001b[39m), \u001b[39m3\u001b[39m)\n\u001b[1;32m    243\u001b[0m \u001b[39mreturn\u001b[39;00m fscope\u001b[39m.\u001b[39mret(retval_, do_return)\n",
      "File \u001b[0;32m/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_filezlsyltsb.py:187\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf____call__.<locals>.else_body_11\u001b[0;34m()\u001b[0m\n\u001b[1;32m    185\u001b[0m ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m_autocast, if_body_7, else_body_7, get_state_7, set_state_7, (\u001b[39m'\u001b[39m\u001b[39minputs\u001b[39m\u001b[39m'\u001b[39m,), \u001b[39m1\u001b[39m)\n\u001b[1;32m    186\u001b[0m \u001b[39mwith\u001b[39;00m ag__\u001b[39m.\u001b[39mld(autocast_variable)\u001b[39m.\u001b[39menable_auto_cast_variables(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m_compute_dtype_object):\n\u001b[0;32m--> 187\u001b[0m     outputs \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(call_fn), (ag__\u001b[39m.\u001b[39mld(inputs),) \u001b[39m+\u001b[39m \u001b[39mtuple\u001b[39m(ag__\u001b[39m.\u001b[39mld(args)), \u001b[39mdict\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mag__\u001b[39m.\u001b[39mld(kwargs)), fscope)\n\u001b[1;32m    189\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_state_8\u001b[39m():\n\u001b[1;32m    190\u001b[0m     \u001b[39mreturn\u001b[39;00m ()\n",
      "File \u001b[0;32m/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_file98v_l434.py:162\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__inject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m     display_name \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mdisplay_name\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    161\u001b[0m     ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mld(arguments_context), if_body_5, else_body_5, get_state_6, set_state_6, (\u001b[39m'\u001b[39m\u001b[39mnew_e\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39marguments_context\u001b[39m\u001b[39m'\u001b[39m), \u001b[39m1\u001b[39m)\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mraise\u001b[39;00m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(new_e)\u001b[39m.\u001b[39mwith_traceback, (ag__\u001b[39m.\u001b[39mld(e)\u001b[39m.\u001b[39m__traceback__,), \u001b[39mNone\u001b[39;00m, fscope_1) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    164\u001b[0m     signature \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39msignature\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_file98v_l434.py:34\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__inject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     do_return_1 \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m     retval__1 \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(fn), \u001b[39mtuple\u001b[39m(ag__\u001b[39m.\u001b[39mld(args)), \u001b[39mdict\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mag__\u001b[39m.\u001b[39mld(kwargs)), fscope_1)\n\u001b[1;32m     35\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     do_return_1 \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_filettvklrq4.py:37\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(func), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m),), \u001b[39mdict\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mag__\u001b[39m.\u001b[39mld(unpacked_inputs)), fscope)\n\u001b[1;32m     38\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_fileoa_8znbw.py:65\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, encoder_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m     63\u001b[0m input_shape \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39minput_shape\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     64\u001b[0m err_msg_prefix \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39merr_msg_prefix\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mand_(\u001b[39mlambda\u001b[39;00m : ag__\u001b[39m.\u001b[39mld(input_ids) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39mlambda\u001b[39;00m : ag__\u001b[39m.\u001b[39mld(inputs_embeds) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m), if_body_2, else_body_2, get_state_2, set_state_2, (\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39minput_shape\u001b[39m\u001b[39m'\u001b[39m), \u001b[39m2\u001b[39m)\n\u001b[1;32m     67\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_state_3\u001b[39m():\n\u001b[1;32m     68\u001b[0m     \u001b[39mreturn\u001b[39;00m (inputs_embeds,)\n",
      "File \u001b[0;32m/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_fileoa_8znbw.py:62\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call.<locals>.else_body_2\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m input_shape \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39minput_shape\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     61\u001b[0m err_msg_prefix \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39merr_msg_prefix\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 62\u001b[0m ag__\u001b[39m.\u001b[39;49mif_stmt(ag__\u001b[39m.\u001b[39;49mld(input_ids) \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, if_body_1, else_body_1, get_state_1, set_state_1, (\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39minput_shape\u001b[39;49m\u001b[39m'\u001b[39;49m), \u001b[39m2\u001b[39;49m)\n",
      "File \u001b[0;32m/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_fileoa_8znbw.py:59\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call.<locals>.else_body_2.<locals>.else_body_1\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m input_shape \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39minput_shape\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     58\u001b[0m err_msg_prefix \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39merr_msg_prefix\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 59\u001b[0m ag__\u001b[39m.\u001b[39;49mif_stmt(ag__\u001b[39m.\u001b[39;49mld(inputs_embeds) \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, if_body, else_body, get_state, set_state, (\u001b[39m'\u001b[39;49m\u001b[39minput_shape\u001b[39;49m\u001b[39m'\u001b[39;49m,), \u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_fileoa_8znbw.py:56\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call.<locals>.else_body_2.<locals>.else_body_1.<locals>.else_body\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mnonlocal\u001b[39;00m input_shape\n\u001b[1;32m     55\u001b[0m err_msg_prefix \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mif_exp(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mis_decoder, \u001b[39mlambda\u001b[39;00m : \u001b[39m'\u001b[39m\u001b[39mdecoder_\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mlambda\u001b[39;00m : \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mself.is_decoder\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[39mraise\u001b[39;00m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mValueError\u001b[39;00m), (\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mYou have to specify either \u001b[39m\u001b[39m{\u001b[39;00mag__\u001b[39m.\u001b[39mld(err_msg_prefix)\u001b[39m}\u001b[39;00m\u001b[39minput_ids or \u001b[39m\u001b[39m{\u001b[39;00mag__\u001b[39m.\u001b[39mld(err_msg_prefix)\u001b[39m}\u001b[39;00m\u001b[39minputs_embeds\u001b[39m\u001b[39m'\u001b[39m,), \u001b[39mNone\u001b[39;00m, fscope)\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/sindhumadhavan/GitHub/transformers/.venv/lib/python3.9/site-packages/tf_keras/src/engine/training.py\", line 2436, in predict_function  *\n        return step_function(self, iterator)\n    File \"/Users/sindhumadhavan/GitHub/transformers/.venv/lib/python3.9/site-packages/tf_keras/src/engine/training.py\", line 2409, in run_step  *\n        outputs = model.predict_step(data)\n    File \"/Users/sindhumadhavan/GitHub/transformers/.venv/lib/python3.9/site-packages/tf_keras/src/engine/training.py\", line 2377, in predict_step  *\n        return self(x, training=False)\n    File \"/Users/sindhumadhavan/GitHub/transformers/.venv/lib/python3.9/site-packages/tf_keras/src/engine/training.py\", line 558, in error_handler  *\n        return fn(*args, **kwargs)\n    File \"/Users/sindhumadhavan/GitHub/transformers/.venv/lib/python3.9/site-packages/tf_keras/src/engine/training.py\", line 588, in __call__  *\n        return super().__call__(*args, **kwargs)\n    File \"/Users/sindhumadhavan/GitHub/transformers/.venv/lib/python3.9/site-packages/tf_keras/src/engine/training.py\", line 558, in error_handler  *\n        return fn(*args, **kwargs)\n    File \"/Users/sindhumadhavan/GitHub/transformers/.venv/lib/python3.9/site-packages/tf_keras/src/engine/base_layer.py\", line 1136, in __call__  *\n        outputs = call_fn(inputs, *args, **kwargs)\n    File \"/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_file98v_l434.py\", line 162, in error_handler\n        raise ag__.converted_call(ag__.ld(new_e).with_traceback, (ag__.ld(e).__traceback__,), None, fscope_1) from None\n    File \"/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_file98v_l434.py\", line 34, in error_handler\n        retval__1 = ag__.converted_call(ag__.ld(fn), tuple(ag__.ld(args)), dict(**ag__.ld(kwargs)), fscope_1)\n    File \"/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_filettvklrq4.py\", line 37, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_fileoa_8znbw.py\", line 65, in tf__call\n        ag__.if_stmt(ag__.and_(lambda : ag__.ld(input_ids) is not None, lambda : ag__.ld(inputs_embeds) is not None), if_body_2, else_body_2, get_state_2, set_state_2, ('input_ids', 'input_shape'), 2)\n    File \"/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_fileoa_8znbw.py\", line 62, in else_body_2\n        ag__.if_stmt(ag__.ld(input_ids) is not None, if_body_1, else_body_1, get_state_1, set_state_1, ('input_ids', 'input_shape'), 2)\n    File \"/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_fileoa_8znbw.py\", line 59, in else_body_1\n        ag__.if_stmt(ag__.ld(inputs_embeds) is not None, if_body, else_body, get_state, set_state, ('input_shape',), 1)\n    File \"/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_fileoa_8znbw.py\", line 56, in else_body\n        raise ag__.converted_call(ag__.ld(ValueError), (f'You have to specify either {ag__.ld(err_msg_prefix)}input_ids or {ag__.ld(err_msg_prefix)}inputs_embeds',), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'tft5_for_conditional_generation' (type TFT5ForConditionalGeneration).\n    \n    in user code:\n    \n        File \"/Users/sindhumadhavan/GitHub/transformers/.venv/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\", line 1404, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/Users/sindhumadhavan/GitHub/transformers/.venv/lib/python3.9/site-packages/transformers/models/t5/modeling_tf_t5.py\", line 1464, in call  *\n            decoder_outputs = self.decoder(\n        File \"/Users/sindhumadhavan/GitHub/transformers/.venv/lib/python3.9/site-packages/tf_keras/src/engine/training.py\", line 558, in error_handler  *\n            return fn(*args, **kwargs)\n        File \"/Users/sindhumadhavan/GitHub/transformers/.venv/lib/python3.9/site-packages/tf_keras/src/engine/base_layer.py\", line 1136, in __call__  *\n            outputs = call_fn(inputs, *args, **kwargs)\n        File \"/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_file98v_l434.py\", line 162, in error_handler  **\n            raise ag__.converted_call(ag__.ld(new_e).with_traceback, (ag__.ld(e).__traceback__,), None, fscope_1) from None\n        File \"/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_file98v_l434.py\", line 34, in error_handler\n            retval__1 = ag__.converted_call(ag__.ld(fn), tuple(ag__.ld(args)), dict(**ag__.ld(kwargs)), fscope_1)\n        File \"/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_filettvklrq4.py\", line 37, in tf__run_call_with_unpacked_inputs  **\n            retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n        File \"/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_fileoa_8znbw.py\", line 65, in tf__call  **\n            ag__.if_stmt(ag__.and_(lambda : ag__.ld(input_ids) is not None, lambda : ag__.ld(inputs_embeds) is not None), if_body_2, else_body_2, get_state_2, set_state_2, ('input_ids', 'input_shape'), 2)\n        File \"/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_fileoa_8znbw.py\", line 62, in else_body_2\n            ag__.if_stmt(ag__.ld(input_ids) is not None, if_body_1, else_body_1, get_state_1, set_state_1, ('input_ids', 'input_shape'), 2)\n        File \"/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_fileoa_8znbw.py\", line 59, in else_body_1\n            ag__.if_stmt(ag__.ld(inputs_embeds) is not None, if_body, else_body, get_state, set_state, ('input_shape',), 1)\n        File \"/var/folders/tm/6zqjkbdn2256qmx7vjl849240000gn/T/__autograph_generated_fileoa_8znbw.py\", line 56, in else_body\n            raise ag__.converted_call(ag__.ld(ValueError), (f'You have to specify either {ag__.ld(err_msg_prefix)}input_ids or {ag__.ld(err_msg_prefix)}inputs_embeds',), None, fscope)\n    \n        ValueError: Exception encountered when calling layer 'decoder' (type TFT5MainLayer).\n        \n        in user code:\n        \n            File \"/Users/sindhumadhavan/GitHub/transformers/.venv/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\", line 1404, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"/Users/sindhumadhavan/GitHub/transformers/.venv/lib/python3.9/site-packages/transformers/models/t5/modeling_tf_t5.py\", line 763, in call  *\n                raise ValueError(f\"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds\")\n        \n            ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds\n        \n        \n        Call arguments received by layer 'decoder' (type TFT5MainLayer):\n          â€¢ input_ids=None\n          â€¢ attention_mask=None\n          â€¢ encoder_hidden_states=tf.Tensor(shape=(16, 1024, 512), dtype=float32)\n          â€¢ encoder_attention_mask=tf.Tensor(shape=(16, 1024), dtype=int32)\n          â€¢ inputs_embeds=None\n          â€¢ head_mask=None\n          â€¢ encoder_head_mask=None\n          â€¢ past_key_values=None\n          â€¢ use_cache=True\n          â€¢ output_attentions=False\n          â€¢ output_hidden_states=False\n          â€¢ return_dict=True\n          â€¢ training=False\n    \n    \n    Call arguments received by layer 'tft5_for_conditional_generation' (type TFT5ForConditionalGeneration):\n      â€¢ input_ids={'input_ids': 'tf.Tensor(shape=(16, 1024), dtype=int64)', 'attention_mask': 'tf.Tensor(shape=(16, 1024), dtype=int64)'}\n      â€¢ attention_mask=None\n      â€¢ decoder_input_ids=None\n      â€¢ decoder_attention_mask=None\n      â€¢ head_mask=None\n      â€¢ decoder_head_mask=None\n      â€¢ encoder_outputs=None\n      â€¢ past_key_values=None\n      â€¢ inputs_embeds=None\n      â€¢ decoder_inputs_embeds=None\n      â€¢ labels=None\n      â€¢ use_cache=None\n      â€¢ output_attentions=None\n      â€¢ output_hidden_states=None\n      â€¢ return_dict=None\n      â€¢ training=False\n"
     ]
    }
   ],
   "source": [
    "model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Inference](https://huggingface.co/docs/transformers/tasks/summarization#inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"summarize: The Inflation Reduction Act lowers prescription drug costs, health care costs, and energy costs. It's the most aggressive action on tackling the climate crisis in American history, which will lift up American workers and create good-paying, union jobs across the country. It'll lower the deficit and ask the ultra-wealthy and corporations to pay their fair share. And no one making under $400,000 per year will pay a penny more in taxes.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "sanoosha94/my_awesome_billsum_model does not appear to have a file named config.json. Checkout 'https://huggingface.co/sanoosha94/my_awesome_billsum_model/main' for available files.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/GitHub/transformers/.venv/lib/python3.9/site-packages/huggingface_hub/utils/_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     response\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[1;32m    305\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/GitHub/transformers/.venv/lib/python3.9/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/sanoosha94/my_awesome_billsum_model/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mEntryNotFoundError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/GitHub/transformers/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:398\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    399\u001b[0m         path_or_repo_id,\n\u001b[1;32m    400\u001b[0m         filename,\n\u001b[1;32m    401\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    402\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m    403\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    404\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    405\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    406\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    407\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    408\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    409\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    410\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    411\u001b[0m     )\n\u001b[1;32m    412\u001b[0m \u001b[39mexcept\u001b[39;00m GatedRepoError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/GitHub/transformers/.venv/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:119\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 119\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/GitHub/transformers/.venv/lib/python3.9/site-packages/huggingface_hub/file_download.py:1261\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, headers, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1260\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1261\u001b[0m     metadata \u001b[39m=\u001b[39m get_hf_file_metadata(\n\u001b[1;32m   1262\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m   1263\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   1264\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1265\u001b[0m         timeout\u001b[39m=\u001b[39;49metag_timeout,\n\u001b[1;32m   1266\u001b[0m         library_name\u001b[39m=\u001b[39;49mlibrary_name,\n\u001b[1;32m   1267\u001b[0m         library_version\u001b[39m=\u001b[39;49mlibrary_version,\n\u001b[1;32m   1268\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m   1269\u001b[0m     )\n\u001b[1;32m   1270\u001b[0m \u001b[39mexcept\u001b[39;00m EntryNotFoundError \u001b[39mas\u001b[39;00m http_error:\n\u001b[1;32m   1271\u001b[0m     \u001b[39m# Cache the non-existence of the file and raise\u001b[39;00m\n",
      "File \u001b[0;32m~/GitHub/transformers/.venv/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:119\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 119\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/GitHub/transformers/.venv/lib/python3.9/site-packages/huggingface_hub/file_download.py:1674\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1673\u001b[0m \u001b[39m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1674\u001b[0m r \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m   1675\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mHEAD\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1676\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m   1677\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1678\u001b[0m     allow_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1679\u001b[0m     follow_relative_redirects\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1680\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1681\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m   1682\u001b[0m )\n\u001b[1;32m   1683\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/GitHub/transformers/.venv/lib/python3.9/site-packages/huggingface_hub/file_download.py:369\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[39mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 369\u001b[0m     response \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m    370\u001b[0m         method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    371\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    372\u001b[0m         follow_relative_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    373\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams,\n\u001b[1;32m    374\u001b[0m     )\n\u001b[1;32m    376\u001b[0m     \u001b[39m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    377\u001b[0m     \u001b[39m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m~/GitHub/transformers/.venv/lib/python3.9/site-packages/huggingface_hub/file_download.py:393\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    392\u001b[0m response \u001b[39m=\u001b[39m get_session()\u001b[39m.\u001b[39mrequest(method\u001b[39m=\u001b[39mmethod, url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[0;32m--> 393\u001b[0m hf_raise_for_status(response)\n\u001b[1;32m    394\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/GitHub/transformers/.venv/lib/python3.9/site-packages/huggingface_hub/utils/_errors.py:315\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    314\u001b[0m     message \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m Client Error.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEntry Not Found for url: \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39murl\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 315\u001b[0m     \u001b[39mraise\u001b[39;00m EntryNotFoundError(message, response) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[39melif\u001b[39;00m error_code \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mGatedRepo\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[0;31mEntryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-660a36ce-24c500bc1a996a666c2d773a;0365c614-62d4-4e1b-a73f-d0e2f089fb12)\n\nEntry Not Found for url: https://huggingface.co/sanoosha94/my_awesome_billsum_model/resolve/main/config.json.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m summarizer \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msummarization\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msanoosha94/my_awesome_billsum_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# summarizer = pipeline(\"summarization\", model=\"stevhliu/my_awesome_billsum_model\")\u001b[39;00m\n\u001b[1;32m      3\u001b[0m summarizer(text)\n",
      "File \u001b[0;32m~/GitHub/transformers/.venv/lib/python3.9/site-packages/transformers/pipelines/__init__.py:815\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    812\u001b[0m                 adapter_config \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(f)\n\u001b[1;32m    813\u001b[0m                 model \u001b[39m=\u001b[39m adapter_config[\u001b[39m\"\u001b[39m\u001b[39mbase_model_name_or_path\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m--> 815\u001b[0m     config \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    816\u001b[0m         model, _from_pipeline\u001b[39m=\u001b[39;49mtask, code_revision\u001b[39m=\u001b[39;49mcode_revision, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs\n\u001b[1;32m    817\u001b[0m     )\n\u001b[1;32m    818\u001b[0m     hub_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39m_commit_hash\n\u001b[1;32m    820\u001b[0m custom_tasks \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m~/GitHub/transformers/.venv/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py:1138\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1135\u001b[0m trust_remote_code \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mtrust_remote_code\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m   1136\u001b[0m code_revision \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mcode_revision\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m-> 1138\u001b[0m config_dict, unused_kwargs \u001b[39m=\u001b[39m PretrainedConfig\u001b[39m.\u001b[39;49mget_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1139\u001b[0m has_remote_code \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mAutoConfig\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1140\u001b[0m has_local_code \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[0;32m~/GitHub/transformers/.venv/lib/python3.9/site-packages/transformers/configuration_utils.py:631\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    629\u001b[0m original_kwargs \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    630\u001b[0m \u001b[39m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_get_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict:\n\u001b[1;32m    633\u001b[0m     original_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m config_dict[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/GitHub/transformers/.venv/lib/python3.9/site-packages/transformers/configuration_utils.py:686\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    682\u001b[0m configuration_file \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39m_configuration_file\u001b[39m\u001b[39m\"\u001b[39m, CONFIG_NAME)\n\u001b[1;32m    684\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    685\u001b[0m     \u001b[39m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 686\u001b[0m     resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m    687\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m    688\u001b[0m         configuration_file,\n\u001b[1;32m    689\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    690\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    691\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    692\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    693\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    694\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    695\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    696\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    697\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m    698\u001b[0m         _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m    699\u001b[0m     )\n\u001b[1;32m    700\u001b[0m     commit_hash \u001b[39m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    701\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    702\u001b[0m     \u001b[39m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m     \u001b[39m# the original exception.\u001b[39;00m\n",
      "File \u001b[0;32m~/GitHub/transformers/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:452\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[39mif\u001b[39;00m revision \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    451\u001b[0m         revision \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmain\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 452\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    453\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m does not appear to have a file named \u001b[39m\u001b[39m{\u001b[39;00mfull_filename\u001b[39m}\u001b[39;00m\u001b[39m. Checkout \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    454\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mrevision\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m for available files.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    455\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    457\u001b[0m     resolved_file \u001b[39m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "\u001b[0;31mOSError\u001b[0m: sanoosha94/my_awesome_billsum_model does not appear to have a file named config.json. Checkout 'https://huggingface.co/sanoosha94/my_awesome_billsum_model/main' for available files."
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"sanoosha94/my_awesome_billsum_model\")\n",
    "# summarizer = pipeline(\"summarization\", model=\"stevhliu/my_awesome_billsum_model\")\n",
    "summarizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6da26a5b0481d39b6fe16099ce6f4dea8f8570d0d73dc88affaf7d040d2c8a3b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
